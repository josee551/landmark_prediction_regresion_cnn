# DocumentaciÃ³n TÃ©cnica: RegresiÃ³n de Landmarks con ResNet-18

## ğŸ“‹ Ãndice
1. [Fundamentos MatemÃ¡ticos](#fundamentos-matemÃ¡ticos)
2. [Arquitectura del Modelo](#arquitectura-del-modelo)
3. [Pipeline de Datos](#pipeline-de-datos)
4. [Algoritmos de Entrenamiento](#algoritmos-de-entrenamiento)
5. [MÃ©tricas de EvaluaciÃ³n](#mÃ©tricas-de-evaluaciÃ³n)
6. [Ensemble Learning](#ensemble-learning)
7. [Optimizaciones Implementadas](#optimizaciones-implementadas)
8. [AnÃ¡lisis de Complejidad](#anÃ¡lisis-de-complejidad)

---

## ğŸ§® Fundamentos MatemÃ¡ticos

### ğŸ“ Espacio de Coordenadas y NormalizaciÃ³n

#### TransformaciÃ³n de Coordenadas
El modelo opera en coordenadas normalizadas para generalizaciÃ³n y estabilidad numÃ©rica:

```
PÃ­xeles â†’ NormalizaciÃ³n:
x_norm = x_pixel / width_original     âˆˆ [0, 1]
y_norm = y_pixel / height_original    âˆˆ [0, 1]

DesnormalizaciÃ³n â†’ PÃ­xeles:
x_pixel = x_norm Ã— width_target
y_pixel = y_norm Ã— height_target
```

#### RepresentaciÃ³n Vectorial
Cada imagen tiene 15 landmarks representados como un vector de 30 dimensiones:

```
L = [xâ‚, yâ‚, xâ‚‚, yâ‚‚, ..., xâ‚â‚…, yâ‚â‚…] âˆˆ [0,1]Â³â°

donde cada landmark k:
L[2k-2] = x_k  (coordenada X del landmark k)
L[2k-1] = y_k  (coordenada Y del landmark k)
```

### ğŸ¯ FunciÃ³n de PÃ©rdida

#### Mean Squared Error (MSE)
FunciÃ³n de pÃ©rdida principal para regresiÃ³n de landmarks:

```
L(Î¸) = (1/N) Î£áµ¢â‚Œâ‚á´º Î£â±¼â‚Œâ‚Â³â° (Å·áµ¢â±¼ - yáµ¢â±¼)Â²

donde:
- N = tamaÃ±o del batch
- Î¸ = parÃ¡metros del modelo
- Å·áµ¢â±¼ = predicciÃ³n j-Ã©sima de la muestra i
- yáµ¢â±¼ = ground truth j-Ã©simo de la muestra i
```

#### Gradientes de la PÃ©rdida
El gradiente con respecto a las predicciones:

```
âˆ‚L/âˆ‚Å·áµ¢â±¼ = (2/N) Ã— (Å·áµ¢â±¼ - yáµ¢â±¼)

Esto proporciona un gradiente proporcional al error,
facilitando la convergencia hacia landmarks precisos.
```

### ğŸ”„ Data Augmentation MatemÃ¡tico

#### Flip Horizontal
ReflexiÃ³n sobre el eje vertical preservando la anatomÃ­a:

```
Para flip horizontal:
x_new = 1.0 - x_original  (reflexiÃ³n en [0,1])
y_new = y_original        (eje Y sin cambios)

Matriz de transformaciÃ³n:
T_flip = [-1  0  1]
         [ 0  1  0]
         [ 0  0  1]
```

#### RotaciÃ³n 2D
RotaciÃ³n aleatoria Â±15Â° alrededor del centro de la imagen:

```
Î¸ ~ Uniform(-15Â°, +15Â°)

Matriz de rotaciÃ³n:
R(Î¸) = [cos(Î¸)  -sin(Î¸)]
       [sin(Î¸)   cos(Î¸)]

AplicaciÃ³n (centrado en 0.5, 0.5):
[x_new] = R(Î¸) Ã— [x_orig - 0.5] + [0.5]
[y_new]          [y_orig - 0.5]   [0.5]
```

#### Transformaciones FotomÃ©tricas
Ajustes de brillo y contraste preservando landmarks:

```
Brillo: I_new = I_original + Î±, Î± ~ Uniform(-0.4, +0.4)
Contraste: I_new = Î² Ã— I_original, Î² ~ Uniform(0.6, 1.4)

Las coordenadas de landmarks no se afectan por estos cambios.
```

---

## ğŸ—ï¸ Arquitectura del Modelo

### ğŸ§  ResNet-18 Base

#### Backbone Preentrenado
```
ResNet-18 (ImageNet pretrained):
â”œâ”€â”€ conv1: Conv2d(3, 64, 7Ã—7, stride=2) + BatchNorm + ReLU + MaxPool
â”œâ”€â”€ layer1: 2 Ã— BasicBlock(64)
â”œâ”€â”€ layer2: 2 Ã— BasicBlock(128, stride=2)
â”œâ”€â”€ layer3: 2 Ã— BasicBlock(256, stride=2)
â”œâ”€â”€ layer4: 2 Ã— BasicBlock(512, stride=2)
â””â”€â”€ avgpool: AdaptiveAvgPool2d(1,1) â†’ 512 features

Total parÃ¡metros backbone: 11,176,512
```

#### BasicBlock Residual
```
BasicBlock(in_channels, out_channels):
x â†’ Conv2d(3Ã—3) â†’ BatchNorm â†’ ReLU â†’ Conv2d(3Ã—3) â†’ BatchNorm
â†“                                                      â†“
identity (o skip connection si dim cambia)              +
                                                       â†“
                                                     ReLU

EcuaciÃ³n matemÃ¡tica:
y = F(x, {Wáµ¢}) + x    (si dimensiones coinciden)
y = F(x, {Wáµ¢}) + WsÃ—x (si hay cambio dimensional)
```

### ğŸ¯ Cabeza de RegresiÃ³n Personalizada

#### Arquitectura de la Cabeza
```
Custom Regression Head:
512 features â†’ Dropout(0.5) â†’ Linear(512, 512) â†’ ReLU
            â†’ Dropout(0.25) â†’ Linear(512, 256) â†’ ReLU
            â†’ Dropout(0.125) â†’ Linear(256, 30) â†’ Sigmoid

ParÃ¡metros de la cabeza: 401,694
```

#### FunciÃ³n de ActivaciÃ³n Sigmoid
```
Ïƒ(x) = 1 / (1 + e^(-x))

Propiedades:
- Rango: (0, 1) - perfecto para coordenadas normalizadas
- Diferenciable: Ïƒ'(x) = Ïƒ(x)(1 - Ïƒ(x))
- SaturaciÃ³n gradual: previene overshooting
```

#### Dropout Progresivo
```
Dropout rates: [0.5, 0.25, 0.125]

FunciÃ³n Dropout:
y = x / (1 - p) Ã— mask, donde mask ~ Bernoulli(1 - p)

Rationale: Mayor dropout al inicio (features generales),
menor dropout al final (features especÃ­ficas).
```

### ğŸ“Š Conteo de ParÃ¡metros

#### DistribuciÃ³n de ParÃ¡metros
```
Componente              | ParÃ¡metros    | Porcentaje
------------------------|---------------|------------
Backbone ResNet-18      | 11,176,512    | 96.5%
Linear 512â†’512 + bias   | 262,656       | 2.3%
Linear 512â†’256 + bias   | 131,328       | 1.1%
Linear 256â†’30 + bias    | 7,710         | 0.1%
------------------------|---------------|------------
TOTAL                   | 11,578,206    | 100%
```

#### ParÃ¡metros Entrenables por Fase
```
Fase 1 (freeze_backbone=True):
- Entrenables: 401,694 (3.5%)
- Congelados: 11,176,512 (96.5%)

Fase 2 (freeze_backbone=False):
- Entrenables: 11,578,206 (100%)
- Congelados: 0 (0%)
```

---

## ğŸ”„ Pipeline de Datos

### ğŸ“¥ Carga y Procesamiento

#### Dataset Loading Algorithm
```python
class LandmarkDataset(Dataset):
    def __init__(self, annotations_file, images_dir, transform=None):
        # 1. Cargar CSV con pandas
        self.annotations = pd.read_csv(annotations_file, header=None)

        # 2. Parsear columnas: [ID, x1, y1, ..., x15, y15, filename]
        self.landmarks = self.annotations.iloc[:, 1:31].values  # 30 coords
        self.filenames = self.annotations.iloc[:, 31].values    # nombres

        # 3. Validar integridad
        self._validate_data_integrity()

    def __getitem__(self, idx):
        # 1. Cargar imagen
        image = cv2.imread(image_path)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # 2. Normalizar landmarks [pÃ­xeles] â†’ [0,1]
        h, w = image_rgb.shape[:2]
        landmarks = self.landmarks[idx].copy()
        landmarks[::2] /= w    # coordenadas X
        landmarks[1::2] /= h   # coordenadas Y

        # 3. Aplicar transformaciones
        if self.transform:
            image_tensor, landmarks_tensor = self.transform(image_rgb, landmarks)

        return image_tensor, landmarks_tensor, metadata
```

#### Data Splitting Strategy
```python
def create_splits(total_size, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):
    """
    DivisiÃ³n estratificada reproducible
    """
    np.random.seed(seed)
    indices = np.random.permutation(total_size)

    train_size = int(total_size * train_ratio)
    val_size = int(total_size * val_ratio)

    train_indices = indices[:train_size]
    val_indices = indices[train_size:train_size + val_size]
    test_indices = indices[train_size + val_size:]

    return train_indices, val_indices, test_indices

# Splits reales del proyecto:
# Total: 999 imÃ¡genes
# Train: 669 (70%), Val: 144 (15%), Test: 144 (15%)
```

### ğŸ–¼ï¸ Transformaciones de Imagen

#### Pipeline de Preprocesamiento
```python
def get_transforms(image_size=(224, 224), is_training=True):
    if is_training:
        return Compose([
            LandmarkRandomHorizontalFlip(p=0.7),      # 70% probabilidad
            LandmarkRandomRotation(degrees=15),        # Â±15 grados
            LandmarkColorJitter(                       # Ajustes fotomÃ©tricos
                brightness=0.4,                        # Â±40%
                contrast=0.4                           # Â±40%
            ),
            LandmarkResize(image_size),                # RedimensiÃ³n a 224Ã—224
            LandmarkToTensor(),                        # Numpy â†’ Tensor
            LandmarkNormalize(                         # NormalizaciÃ³n ImageNet
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])
    else:
        return Compose([
            LandmarkResize(image_size),
            LandmarkToTensor(),
            LandmarkNormalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])
```

#### TransformaciÃ³n Landmark-Aware
```python
class LandmarkRandomHorizontalFlip:
    def __call__(self, image, landmarks):
        if random.random() < self.p:
            # Flip imagen
            image = cv2.flip(image, 1)

            # Flip coordenadas X de landmarks
            landmarks_copy = landmarks.copy()
            landmarks_copy[::2] = 1.0 - landmarks_copy[::2]

            return image, landmarks_copy
        return image, landmarks

class LandmarkRandomRotation:
    def __call__(self, image, landmarks):
        angle = random.uniform(-self.degrees, self.degrees)

        # Rotar imagen
        center = (image.shape[1] // 2, image.shape[0] // 2)
        M = cv2.getRotationMatrix2D(center, angle, 1.0)
        image_rotated = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))

        # Rotar landmarks
        landmarks_rotated = self._rotate_landmarks(landmarks, angle, center)

        return image_rotated, landmarks_rotated
```

---

## ğŸ‹ï¸ Algoritmos de Entrenamiento

### ğŸ¯ Transfer Learning en 2 Fases

#### Fase 1: Head Training Algorithm
```python
def train_phase1(model, dataloader, optimizer, criterion, device):
    """
    Entrenamiento solo de la cabeza con backbone congelado
    """
    # 1. Congelar backbone
    model.freeze_backbone()

    # 2. Verificar parÃ¡metros entrenables
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ParÃ¡metros entrenables: {trainable_params:,}")  # 401,694

    # 3. Loop de entrenamiento
    model.train()
    for epoch in range(epochs):
        epoch_loss = 0.0

        for batch_idx, (images, landmarks, _) in enumerate(dataloader):
            images, landmarks = images.to(device), landmarks.to(device)

            optimizer.zero_grad()
            predictions = model(images)
            loss = criterion(predictions, landmarks)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(dataloader)
        print(f"Ã‰poca {epoch+1}: Loss = {avg_loss:.6f}")
```

#### Fase 2: Fine-tuning Algorithm
```python
def train_phase2(model, dataloader, device):
    """
    Fine-tuning completo con learning rates diferenciados
    """
    # 1. Descongelar backbone
    model.unfreeze_backbone()

    # 2. Configurar optimizador con LR diferenciados
    param_groups = [
        {
            'params': model.get_backbone_parameters(),
            'lr': 0.00002,          # LR bajo para preservar features ImageNet
            'name': 'backbone'
        },
        {
            'params': model.get_head_parameters(),
            'lr': 0.0002,           # LR alto para especializaciÃ³n (10Ã— backbone)
            'name': 'head'
        }
    ]
    optimizer = torch.optim.Adam(param_groups, weight_decay=0.00005)

    # 3. Scheduler CosineAnnealing
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=epochs, eta_min=0.000002
    )

    # 4. Loop de entrenamiento con gradient clipping
    model.train()
    for epoch in range(epochs):
        for batch_idx, (images, landmarks, _) in enumerate(dataloader):
            images, landmarks = images.to(device), landmarks.to(device)

            optimizer.zero_grad()
            predictions = model(images)
            loss = criterion(predictions, landmarks)
            loss.backward()

            # Gradient clipping para estabilidad
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

        scheduler.step()  # Actualizar learning rates
```

### ğŸ“ˆ Learning Rate Scheduling

#### CosineAnnealingLR Mathematical Formula
```
Î·_t = Î·_min + (Î·_max - Î·_min) Ã— (1 + cos(Ï€t/T)) / 2

donde:
- Î·_t = learning rate en Ã©poca t
- Î·_max = learning rate inicial
- Î·_min = learning rate mÃ­nimo
- T = nÃºmero total de Ã©pocas
- t = Ã©poca actual

ConfiguraciÃ³n del proyecto:
- Backbone: Î·_max = 0.00002, Î·_min = 0.000002
- Head: Î·_max = 0.0002, Î·_min = 0.00002
- T = 55 Ã©pocas
```

#### Early Stopping Algorithm
```python
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0.0001, monitor='val_loss'):
        self.patience = patience
        self.min_delta = min_delta
        self.monitor = monitor
        self.best_score = None
        self.counter = 0

    def __call__(self, current_score):
        if self.best_score is None:
            self.best_score = current_score
        elif current_score < self.best_score - self.min_delta:
            self.best_score = current_score
            self.counter = 0
        else:
            self.counter += 1

        return self.counter >= self.patience
```

---

## ğŸ“Š MÃ©tricas de EvaluaciÃ³n

### ğŸ¯ MÃ©tricas Principales

#### Root Mean Square Error (RMSE)
```
RMSE = âˆš[(1/N) Î£áµ¢â‚Œâ‚á´º (Å·áµ¢ - yáµ¢)Â²]

Para landmarks:
RMSE_normalized = âˆš[(1/(NÃ—30)) Î£áµ¢â‚Œâ‚á´º Î£â±¼â‚Œâ‚Â³â° (Å·áµ¢â±¼ - yáµ¢â±¼)Â²]
RMSE_pixels = RMSE_normalized Ã— 224

Valor del proyecto: RMSE = 9.47 pÃ­xeles
```

#### Mean Absolute Error (MAE)
```
MAE = (1/N) Î£áµ¢â‚Œâ‚á´º |Å·áµ¢ - yáµ¢|

Para landmarks:
MAE_normalized = (1/(NÃ—30)) Î£áµ¢â‚Œâ‚á´º Î£â±¼â‚Œâ‚Â³â° |Å·áµ¢â±¼ - yáµ¢â±¼|
MAE_pixels = MAE_normalized Ã— 224

Valor del proyecto: MAE = 7.15 pÃ­xeles
```

#### Distancia Euclidiana por Landmark
```
Para cada landmark k en muestra i:
d_k^(i) = âˆš[(xÌ‚_k^(i) - x_k^(i))Â² + (Å·_k^(i) - y_k^(i))Â²] Ã— 224

Error promedio por landmark:
dÌ„_k = (1/N) Î£áµ¢â‚Œâ‚á´º d_k^(i)

Error promedio global:
Error_avg = (1/15) Î£â‚–â‚Œâ‚Â¹âµ dÌ„_k = 11.34 pÃ­xeles
```

### ğŸ“ˆ AnÃ¡lisis EstadÃ­stico

#### DistribuciÃ³n de Errores por CategorÃ­a
```python
def analyze_by_category(predictions, targets, categories):
    """
    AnÃ¡lisis estadÃ­stico por categorÃ­a mÃ©dica
    """
    results = {}

    for category in ['COVID', 'Normal', 'Viral_Pneumonia']:
        # Filtrar por categorÃ­a
        mask = [cat == category for cat in categories]
        cat_predictions = predictions[mask]
        cat_targets = targets[mask]

        # Calcular mÃ©tricas
        errors = torch.abs(cat_predictions - cat_targets)
        euclidean_distances = []

        for i in range(len(cat_predictions)):
            pred_coords = cat_predictions[i].reshape(15, 2)
            true_coords = cat_targets[i].reshape(15, 2)
            distances = torch.norm(pred_coords - true_coords, dim=1) * 224
            euclidean_distances.extend(distances.tolist())

        results[category] = {
            'mean_error': np.mean(euclidean_distances),
            'std_error': np.std(euclidean_distances),
            'median_error': np.median(euclidean_distances),
            'p95_error': np.percentile(euclidean_distances, 95),
            'samples': len(cat_predictions)
        }

    return results

# Resultados del proyecto:
# Normal: 10.46 Â± 6.63 pÃ­xeles
# Viral Pneumonia: 11.38 Â± 7.20 pÃ­xeles
# COVID: 13.24 Â± 8.27 pÃ­xeles
```

---

## ğŸ¯ Ensemble Learning

### ğŸ”„ Bootstrap Aggregating (Bagging)

#### Algoritmo de Ensemble
```python
class EnsemblePredictor:
    def __init__(self, model_paths):
        self.models = []
        for path in model_paths:
            model, _ = ResNetLandmarkRegressor.load_from_checkpoint(path)
            model.eval()
            self.models.append(model)

    def predict(self, x, aggregation='mean'):
        """
        PredicciÃ³n ensemble con agregaciÃ³n configurable
        """
        predictions = []

        with torch.no_grad():
            for model in self.models:
                pred = model(x)
                predictions.append(pred)

        # Stack: [num_models, batch_size, 30]
        stacked_preds = torch.stack(predictions, dim=0)

        if aggregation == 'mean':
            return torch.mean(stacked_preds, dim=0)
        elif aggregation == 'median':
            return torch.median(stacked_preds, dim=0)[0]
        elif aggregation == 'weighted_mean':
            weights = self._calculate_weights()
            weighted_preds = stacked_preds * weights.view(-1, 1, 1)
            return torch.sum(weighted_preds, dim=0)

    def _calculate_weights(self):
        """
        Pesos inversamente proporcionales a la pÃ©rdida de validaciÃ³n
        """
        val_losses = [0.002, 0.0018, 0.0025, 0.0022, 0.0019]  # Ejemplo
        inv_losses = [1.0 / (loss + 1e-8) for loss in val_losses]
        weights = torch.tensor(inv_losses)
        return weights / torch.sum(weights)  # Normalizar
```

#### AnÃ¡lisis de Diversidad
```python
def calculate_ensemble_diversity(predictions_list):
    """
    Medir diversidad entre modelos del ensemble
    """
    # Convertir a numpy para anÃ¡lisis
    preds_array = np.array([p.numpy() for p in predictions_list])

    # Varianza promedio entre modelos
    variance = np.var(preds_array, axis=0)
    avg_variance = np.mean(variance)

    # CorrelaciÃ³n promedio entre pares de modelos
    correlations = []
    for i in range(len(predictions_list)):
        for j in range(i+1, len(predictions_list)):
            corr = np.corrcoef(
                preds_array[i].flatten(),
                preds_array[j].flatten()
            )[0, 1]
            correlations.append(corr)

    avg_correlation = np.mean(correlations)

    return {
        'average_variance': avg_variance,
        'average_correlation': avg_correlation,
        'diversity_score': avg_variance / (avg_correlation + 1e-8)
    }

# Resultados del proyecto:
# Diversidad limitada: correlaciÃ³n alta entre modelos (>0.9)
# ExplicaciÃ³n: Solo cambio de seed no genera suficiente diversidad
```

### ğŸ“Š Resultados del Ensemble

#### ComparaciÃ³n Individual vs Ensemble
```
Modelo    | Seed | Error (pÃ­xeles) | Ranking
----------|------|-----------------|--------
Modelo 1  | 123  | 11.55          | 4
Modelo 2  | 42   | 12.14          | 5 (peor)
Modelo 3  | 456  | 10.69          | 1 (mejor)
Modelo 4  | 789  | 11.39          | 3
Modelo 5  | 999  | 11.53          | 2

Ensemble (mean):        10.81 pÃ­xeles
Ensemble (median):      10.81 pÃ­xeles
Ensemble (weighted):    10.82 pÃ­xeles

Mejora vs mejor individual: -0.12 pÃ­xeles (marginal)
```

---

## âš¡ Optimizaciones Implementadas

### ğŸ¯ HiperparÃ¡metros Optimizados

#### Learning Rate Optimization
```python
# ConfiguraciÃ³n ganadora
optimizer_config = {
    'backbone_lr': 0.00002,     # LR bajo para preservar features ImageNet
    'head_lr': 0.0002,          # LR alto para especializaciÃ³n (ratio 10:1)
    'weight_decay': 0.00005,    # Reducido para mayor flexibilidad
    'optimizer': 'adam',        # Adam con Î²â‚=0.9, Î²â‚‚=0.999
}

# JustificaciÃ³n matemÃ¡tica:
# Backbone preentrenado: pequeÃ±os ajustes â†’ LR bajo
# Head aleatorio: aprendizaje desde cero â†’ LR alto
# Ratio 10:1 permite convergencia balanceada
```

#### Data Augmentation Optimization
```python
# ConfiguraciÃ³n agresiva optimizada
augmentation_config = {
    'horizontal_flip': 0.7,     # â†‘40% vs baseline (0.5)
    'rotation': 15,             # â†‘50% vs baseline (10Â°)
    'brightness': 0.4,          # â†‘100% vs baseline (0.2)
    'contrast': 0.4,            # â†‘100% vs baseline (0.2)
}

# Impacto en generalizaciÃ³n:
# Mayor variabilidad â†’ mejor robustez â†’ -8% error
```

#### Batch Size Optimization
```python
# AnÃ¡lisis de batch size vs rendimiento
batch_sizes = [4, 8, 16, 32]
errors = [11.8, 11.34, 11.9, 12.5]  # pÃ­xeles

# Ã“ptimo: batch_size = 8
# RazÃ³n: Balance entre estabilidad y precisiÃ³n de gradientes
# Batch pequeÃ±o â†’ gradientes mÃ¡s ruidosos pero precisos
# Batch grande â†’ gradientes estables pero menos informativos
```

### ğŸ”§ Optimizaciones de CÃ³digo

#### Memory-Efficient Data Loading
```python
class MemoryEfficientDataLoader:
    def __init__(self, dataset, batch_size, num_workers=4, pin_memory=True):
        # Optimizaciones para AMD GPU
        self.dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,       # ParalelizaciÃ³n CPU
            pin_memory=pin_memory,         # OptimizaciÃ³n GPU
            persistent_workers=True,       # Reutilizar workers
            prefetch_factor=2              # Prefetch para pipeline
        )
```

#### Gradient Accumulation (si batch_size < Ã³ptimo)
```python
def train_with_accumulation(model, dataloader, optimizer, criterion,
                          accumulation_steps=4):
    """
    Simular batch_size mayor con acumulaciÃ³n de gradientes
    """
    model.train()
    optimizer.zero_grad()

    for i, (images, landmarks, _) in enumerate(dataloader):
        images, landmarks = images.to(device), landmarks.to(device)

        predictions = model(images)
        loss = criterion(predictions, landmarks)

        # Escalar pÃ©rdida por pasos de acumulaciÃ³n
        loss = loss / accumulation_steps
        loss.backward()

        if (i + 1) % accumulation_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            optimizer.zero_grad()
```

---

## ğŸ“Š AnÃ¡lisis de Complejidad

### â±ï¸ Complejidad Temporal

#### Forward Pass Complexity
```
ResNet-18 Forward Pass:
- Input: (batch_size, 3, 224, 224)
- FLOPs â‰ˆ 1.8 Ã— 10â¹ operaciones por imagen
- GPU AMD RX 6600: ~6000 GFLOPS
- Tiempo estimado: ~0.3ms por imagen

Custom Head:
- Linear layers: O(dâ‚ Ã— dâ‚‚) donde dâ‚, dâ‚‚ son dimensiones
- 512â†’512: 262K FLOPs
- 512â†’256: 131K FLOPs
- 256â†’30: 8K FLOPs
- Total head: ~401K FLOPs (negligible vs backbone)
```

#### Training Time Complexity
```
Fase 1 (2 Ã©pocas):
- Forward + Backward: ~2Ã— Forward FLOPs
- Solo head entrenable: 401K parÃ¡metros
- Tiempo: ~1 minuto (669 muestras Ã— 2 Ã©pocas)

Fase 2 (55 Ã©pocas):
- Forward + Backward: ~2Ã— Forward FLOPs
- Todo el modelo: 11.6M parÃ¡metros
- Tiempo: ~4 minutos (669 muestras Ã— 55 Ã©pocas)

Ensemble (5 modelos):
- 5Ã— tiempo Fase 2: ~20 minutos
- Paralelizable en mÃºltiples GPUs
```

### ğŸ’¾ Complejidad Espacial

#### Memory Usage Analysis
```python
def calculate_memory_usage():
    """
    AnÃ¡lisis de uso de memoria GPU
    """
    memory_breakdown = {
        'model_parameters': 11.6e6 * 4,      # 46.4 MB (float32)
        'activations_per_image': 50e6 * 4,   # 200 MB estimado
        'gradients': 11.6e6 * 4,             # 46.4 MB
        'optimizer_states': 11.6e6 * 8,      # 92.8 MB (Adam: 2Ã— params)
        'batch_data': 8 * 3 * 224 * 224 * 4, # 48.2 MB (batch=8)
    }

    total_mb = sum(memory_breakdown.values()) / (1024**2)
    print(f"Uso estimado de GPU: {total_mb:.1f} MB")

    return memory_breakdown

# GPU AMD RX 6600 (8GB): ~433 MB usados (~5.4% de capacidad)
```

#### Disk Space Requirements
```
Componente              | TamaÃ±o       | DescripciÃ³n
------------------------|--------------|------------------
Dataset original        | ~150 MB      | 999 imÃ¡genes PNG
Checkpoints modelo      | ~45 MB       | phase2_best.pt
Ensemble checkpoints    | ~225 MB      | 5 modelos Ã— 45MB
Logs TensorBoard        | ~10 MB       | MÃ©tricas entrenamiento
Resultados evaluaciÃ³n   | ~20 MB       | CSVs + visualizaciones
Total proyecto          | ~450 MB      | Sin datos intermedios
```

---

## ğŸ” AnÃ¡lisis de Convergencia

### ğŸ“ˆ Curvas de Aprendizaje

#### Fase 1: Head Training Convergence
```
Ã‰poca | Train Loss | Val Loss | Convergencia
------|------------|----------|-------------
1     | 0.0890     | 0.0856   | RÃ¡pida inicial
2     | 0.0234     | 0.0267   | EstabilizaciÃ³n

AnÃ¡lisis: Convergencia rÃ¡pida debido a:
- Solo 401K parÃ¡metros entrenables
- Features preentrenados estables
- LR alto (0.001) para head
```

#### Fase 2: Fine-tuning Convergence
```python
# PatrÃ³n tÃ­pico de convergencia Fase 2
epochs = list(range(1, 56))
train_loss = [0.0267, 0.0198, 0.0165, ..., 0.0012]  # Decreciente
val_loss = [0.0278, 0.0201, 0.0178, ..., 0.0018]    # Con plateau

# DetecciÃ³n de overfitting
overfitting_point = None
for i in range(10, len(val_loss)):
    if val_loss[i] > val_loss[i-5]:  # Loss aumenta durante 5 Ã©pocas
        overfitting_point = i
        break

print(f"Posible overfitting despuÃ©s de Ã©poca: {overfitting_point}")
```

### ğŸ¯ AnÃ¡lisis de Estabilidad

#### Variance Across Random Seeds
```python
def analyze_seed_stability():
    """
    AnÃ¡lisis de estabilidad entre diferentes semillas
    """
    seeds = [42, 123, 456, 789, 999]
    errors = [12.14, 11.55, 10.69, 11.39, 11.53]  # pÃ­xeles

    mean_error = np.mean(errors)      # 11.46 pÃ­xeles
    std_error = np.std(errors)        # 0.58 pÃ­xeles
    cv = std_error / mean_error       # 0.051 (5.1% variaciÃ³n)

    print(f"Estabilidad del modelo:")
    print(f"Error promedio: {mean_error:.2f} Â± {std_error:.2f} pÃ­xeles")
    print(f"Coeficiente de variaciÃ³n: {cv:.3f}")

    # CV < 0.1 indica buena estabilidad
    stability = "BUENA" if cv < 0.1 else "REGULAR" if cv < 0.2 else "MALA"
    print(f"EvaluaciÃ³n estabilidad: {stability}")

analyze_seed_stability()
# Resultado: BUENA estabilidad (CV = 5.1%)
```

---

## ğŸ“ Conclusiones TÃ©cnicas

### âœ… Fortalezas del Modelo

1. **Arquitectura Robusta**: ResNet-18 con transfer learning probado
2. **Entrenamiento Eficiente**: 2 fases optimizan convergencia
3. **GeneralizaciÃ³n SÃ³lida**: Data augmentation agresivo efectivo
4. **Estabilidad Alta**: Baja varianza entre runs (CV = 5.1%)
5. **PrecisiÃ³n ClÃ­nica**: 11.34px cercano a objetivo <10px

### âš ï¸ Limitaciones Identificadas

1. **Capacidad Arquitectural**: ResNet-18 podrÃ­a ser limitante para <10px
2. **Diversidad Ensemble**: Solo random seeds insuficiente para mejora
3. **Landmarks EspecÃ­ficos**: #14 y #15 consistentemente problemÃ¡ticos
4. **Variabilidad CategÃ³rica**: COVID mÃ¡s desafiante (+23% error vs Normal)

### ğŸš€ Optimizaciones Futuras

1. **Arquitectura**: ResNet-34, EfficientNet, Vision Transformers
2. **Loss Functions**: Wing Loss, Focal Loss para landmarks difÃ­ciles
3. **Ensemble Diversity**: Diferentes arquitecturas, augmentation, loss
4. **Attention Mechanisms**: Self-attention para landmarks relacionados
5. **Multi-Scale Training**: Entrenamiento con mÃºltiples resoluciones

---

**ğŸ“Š Estado Final**: El proyecto alcanza **11.34 pÃ­xeles de error promedio**, estableciendo una base sÃ³lida para predicciÃ³n de landmarks mÃ©dicos con precisiÃ³n clÃ­nicamente Ãºtil. La documentaciÃ³n tÃ©cnica proporciona fundamentos matemÃ¡ticos completos para futuras extensiones e investigaciones.